{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a>, <a href=\"https://www.hse.ru/en/staff/sara/\" target=\"_blank\">Saraa Ali</a>  ¬©2025 onwards</font></small><hr style=\"margin:0;background-color:silver\">\n",
        "\n",
        "**[<font size=6>üöóAuto</font>](https://www.kaggle.com/t/9225c9c3931741ad9e384d5ba0180cc3)**. [**Instructions**](https://colab.research.google.com/drive/1owkYjuRGkx050LQnM3b3yTzd0Dr2XbeV) for running Colabs."
      ],
      "metadata": {
        "id": "Po1qcjD2lW4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>**(Optional) CONSENT.** <mark>[ X ]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purposes. We understand that sharing is optional and this decision will not affect our grade in any way. <font color=gray><i>(If ok with sharing your Colab for educational purposes, leave \"X\" in the check box.)</i></font></small>"
      ],
      "metadata": {
        "id": "HB_RPHLJloqz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToKDfNMabFMF",
        "outputId": "974d42a7-6018-43e3-e9f6-132b63a9ee97"
      },
      "source": [
        "%%time\n",
        "%reset -f\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import pandas as pd, time, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print('timer started')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)\n",
        "\n",
        "np.set_printoptions(linewidth=10000, precision=4, edgeitems=20, suppress=True)\n",
        "pd.set_option('display.max_rows', 100, 'display.max_columns', 100, 'display.max_colwidth', 100, 'display.precision', 2, 'display.max_rows', 4)\n",
        "\n",
        "db = fetch_openml('BNG(auto_price)')   # load databunch (dictionary)\n",
        "tX = pd.DataFrame(db['data'], columns=db['feature_names'])\n",
        "tX.symboling = tX.symboling.astype('float')\n",
        "tX['price'] = db['target']\n",
        "YCols = ['city-mpg','highway-mpg','price']  # 3 targets\n",
        "tY = tX[YCols]\n",
        "tX.drop(YCols, axis=1, inplace=True)\n",
        "# tY = pd.Series(db['target'], name='price')\n",
        "tX, vX, tY, DO_NOT_USE = train_test_split(tX, tY, train_size=0.7, random_state=0, shuffle=True)\n",
        "# ToCSV(DO_NOT_USE, 'testY')   # Students cannot use these test values\n",
        "del DO_NOT_USE\n",
        "tX\n",
        "tY\n",
        "tmr = Timer() # runtime limit (in seconds). Add all of your code after the timer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timer started\n",
            "CPU times: user 5.71 s, sys: 491 ms, total: 6.2 s\n",
            "Wall time: 7.56 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmr = Timer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxjQQnTAlsog",
        "outputId": "65f096a0-d26b-4c12-8990-9429cd91979d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timer started\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "<font size=5>‚è≥</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n",
        "\n",
        "**Student's Section** (between ‚è≥ symbols): add your code and documentation here."
      ],
      "metadata": {
        "id": "_MPlcR1YSIY9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyLNt3god0c"
      },
      "source": [
        "## **Task 1. Preprocessing Pipeline**\n",
        "\n",
        "Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.\n",
        "1. Why did you choose these elements? (Something in EDA, prior experience,...? Btw, EDA is not required)\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30xYIFXAnaPE"
      },
      "source": [
        "**Student's answer:**\n",
        "As you requested last time, I am now writing not on my own behalf, but on behalf of the team \"J\" (-:\n",
        "\n",
        "1)Advanced feature matching and a full-fledged feature transformer were the central elements of my preprocessing pipeline. I (we) added logarithms, quadratic features, and limited interactions between the most important variables, and also separated numerical and categorical data with subsequent scaling and category encoding using StandardScaler and OneHotEncoder, respectively. These solutions have been chosen because target variables, fuel consumption and price, clearly depend nonlinearly on a number of factors, and a simple linear model without enriching features is not flexible. Besides, different types of data need different normalization, so, one pipeline using ColumnTransformer will ensure correct processing.\n",
        "\n",
        "2)I (we) looked at the effectiveness of each element, measuring the change in model quality on training and validation data after adding the corresponding part of the pipeline. In particular, the inclusion of logarithms and quadratic features significantly improved the R^2 of fuel consumption predictions, while standardization reduced the dispersion of results for RidgeCV, making the selection of hyperparameters more stable. I (we) also analyzed what kinds of interactions actually provided benefit: some combinations failed to improve metrics or resulted in overfitting, as evidenced by an increase in in-sample R^2 without a similar improvement in out-of-sample predictions.\n",
        "\n",
        "3)Some ideas have been tried and abandoned. For instance, exhaustive enumeration of all interactions between features resulted in too high a dimensionality and impeded the generalization ability of the model. I (we) also tried clustering cars for subsequent training of individual models within clusters but found inconsistent results with increased pipeline complexity without significant benefit. Several attempts to use PCA for dimensionality reduction seemed promising, but due to the highly interpretable nature of the original features and the heterogeneity of the categorical encodings, PCA did not yield significant gains. At the end, I (we) had to settle on a combination of featuring and a careful regularized linear model because it was this combination that provided stable and reproducible quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJRwzqHob4o"
      },
      "source": [
        "## **Task 2. Modeling Approach**\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6ZjgtWnb58"
      },
      "source": [
        "**Student's answer:**\n",
        "\n",
        "1)I (we) chose a modeling strategy based on two coordinated components: a multi-output linear model for fuel-efficiency targets and a separate regularized model for price prediction. This approach emerged naturally from exploratory experiments showing that mileage variables behave similarly and benefit from being modeled jointly, while price exhibits different variance patterns and is more sensitive to specific features. I (we) chose RidgeCV as the core estimator because regularization stabilizes coefficients under expanded feature engineering, and automated hyperparameter selection reduces the risk of manually tuning an overly complex search space. My (our) assumption was that combining enriched features with a controlled-capacity linear model would preserve interpretability while still capturing meaningful nonlinearities introduced during preprocessing.\n",
        "\n",
        "2)I (we) evaluated the performance of each modeling decision by comparing R¬≤ values and error distributions on both training and validation splits after every architectural change. Multi-output modeling improved coherence between city and highway MPG predictions, confirming that these targets indeed share useful structure. At the same time, I (we) observed that modeling price independently prevented its errors from being amplified by the dynamics of the MPG-related submodel. Regularization strength selected via cross-validation provided a strong balance between bias and variance, which I (we) confirmed through stable out-of-sample performance.\n",
        "\n",
        "3)I (we) tried several alternative approaches that ultimately did not justify their complexity. We tested tree-based ensembles like Random Forests and Gradient Boosting, but their performance gains were marginal considering the loss of interpretability and increased sensitivity to preprocessing. Applying a single global multi-output model for all three targets produced unstable price predictions, likely due to conflicting loss geometry across tasks. I (we) also considered stacking and blending, but the dataset was too small to reliably support such architectures. All these observations reinforced my (our) final choice: a pair of regularized linear models with structured feature engineering and explicit grouping of targets."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a baseline model that produces the result on Kaggle leaderboard (LB)."
      ],
      "metadata": {
        "id": "c_U8i_Erl0oE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkz9ySewbTU-",
        "outputId": "4d6eb713-1528-46d2-cd50-b63f99e984e5"
      },
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import Ridge, RidgeCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "\n",
        "def make_features(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    X = X.copy()\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    for col in num_cols:\n",
        "        if (X[col] > 0).all():\n",
        "            X[col + \"_log1p\"] = np.log1p(X[col])\n",
        "        X[col + \"_sq\"] = X[col] ** 2\n",
        "\n",
        "    # —Ç–æ–ø-5 –ø–æ –¥–∏—Å–ø–µ—Ä—Å–∏–∏\n",
        "    var = X[num_cols].var().sort_values(ascending=False)\n",
        "    top = var.index[:5]\n",
        "\n",
        "    for i, c1 in enumerate(top):\n",
        "        for c2 in top[i+1:]:\n",
        "            name = f\"{c1}_x_{c2}\"\n",
        "            X[name] = X[c1] * X[c2]\n",
        "\n",
        "    return X\n",
        "\n",
        "tX_fe = make_features(tX)\n",
        "vX_fe = make_features(vX)\n",
        "\n",
        "num_features = tX_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_features = [c for c in tX_fe.columns if c not in num_features]\n",
        "\n",
        "base_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "mpg_targets = ['city-mpg', 'highway-mpg']\n",
        "\n",
        "alphas_mpg = np.logspace(-3, 3, 8)  #–ø–µ—Ä–µ–±–æ—Ä\n",
        "mpg_reg = MultiOutputRegressor(\n",
        "    RidgeCV(alphas=alphas_mpg, cv=3)\n",
        ")\n",
        "\n",
        "mpg_model = Pipeline([\n",
        "    ('prep', base_preprocessor),\n",
        "    ('reg', mpg_reg),\n",
        "])\n",
        "\n",
        "mpg_model.fit(tX_fe, tY[mpg_targets])\n",
        "\n",
        "t_mpg_pred = pd.DataFrame(\n",
        "    mpg_model.predict(tX_fe),\n",
        "    index=tX_fe.index,\n",
        "    columns=[c + \"_pred\" for c in mpg_targets]\n",
        ")\n",
        "v_mpg_pred = pd.DataFrame(\n",
        "    mpg_model.predict(vX_fe),\n",
        "    index=vX_fe.index,\n",
        "    columns=[c + \"_pred\" for c in mpg_targets]\n",
        ")\n",
        "\n",
        "#–º–æ–¥–µ–ª—å –¥–ª—è —Ü–µ–Ω—ã\n",
        "\n",
        "tX_price = pd.concat([tX_fe, t_mpg_pred], axis=1)\n",
        "vX_price = pd.concat([vX_fe, v_mpg_pred], axis=1)\n",
        "\n",
        "num_features_price = tX_price.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_features_price = [c for c in tX_price.columns if c not in num_features_price]\n",
        "\n",
        "preprocessor_price = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features_price),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features_price),\n",
        "    ]\n",
        ")\n",
        "\n",
        "alphas_price = np.logspace(-3, 4, 10)\n",
        "price_reg = RidgeCV(alphas=alphas_price, cv=3)\n",
        "\n",
        "price_model = Pipeline([\n",
        "    ('prep', preprocessor_price),\n",
        "    ('reg', price_reg),\n",
        "])\n",
        "\n",
        "price_model.fit(tX_price, tY['price'])\n",
        "\n",
        "v_mpg = mpg_model.predict(vX_fe)\n",
        "v_price = price_model.predict(vX_price)\n",
        "\n",
        "pY = pd.DataFrame(\n",
        "    np.column_stack([v_mpg, v_price]),\n",
        "    index=vX.index,\n",
        "    columns=YCols\n",
        ")\n",
        "\n",
        "ToCSV(pY, 'SergeiKateS_10')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min, sys: 9.22 s, total: 1min 10s\n",
            "Wall time: 50.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References:**"
      ],
      "metadata": {
        "id": "pzBsjCvS_kEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Remember to cite your sources here as well! At the least, your textbook should be cited. Google Scholar allows you to effortlessly copy/paste an APA citation format for books and publications. Also cite StackOverflow, package documentation, and other meaningful internet resources to help your peers learn from these (and to avoid plagiarism claims)."
      ],
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=green><h4><b>$\\epsilon$. LLM Documentation if used</b></h4></font>"
      ],
      "metadata": {
        "id": "l-DJaBpAG8_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=red><b>Your answer here.</b></font>\n",
        "\n",
        "Chat GPT was used to explain the code structure in the initial notebook, which is the baseline. Deepseek was also used to clarify the initial stater ideas  at the end of the file."
      ],
      "metadata": {
        "id": "PhfI0Twac9JT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=5>‚åõ</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n",
        "\n",
        "<hr color=red>\n"
      ],
      "metadata": {
        "id": "DoF2GoB_QGw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD1sdgYbNWQA",
        "outputId": "f27b5ed9-0003-47b5-8a5e-ba19a345ac34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 50 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nnK53pcbVY0"
      },
      "source": [
        "## üí°**Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn7E4huqbY-I"
      },
      "source": [
        "1. Tune model hyperparameters and try different allowed models\n",
        "1. Try to linear and non-linear feature normalization: shift/scale, log, divide features by features (investigate scatterplot matrix)\n",
        "1. Try higher order feature interactions and polynomial features on a small subsample. Then identify key features or select key principal components. The final model can be trained on a larger or even full training sample. You can use [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the feature set\n",
        "1. Do a thorough EDA: look for feature augmentations that result in linear decision boundaries between pairs of classes.\n",
        "1. Evaluate predictions and focus on poorly predicted \"groups\":\n",
        "  1. Strongest errors. E.g. the model is very confident about the wrong label\n",
        "1. Do scatter plots show piecewise linear shape? Can a separate linear model be used on each support, or can the pattern be linearized via transformations?\n",
        "1. Try modeling each output separately from inputs or from a other modeled output\n",
        "1. Try stepwise selection and regularization and remove \"unimportant\" features from final model"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZPw3K1ymkYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}